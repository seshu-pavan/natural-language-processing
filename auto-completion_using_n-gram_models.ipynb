{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPYUPEOxHWoGXLlDNoZNtb6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Auto-Completion using N-Gram Models"
      ],
      "metadata": {
        "id": "RkDLpKmxMUnV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WQ-Wc6mnRvP8"
      },
      "outputs": [],
      "source": [
        "!pip install opendatasets -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import opendatasets as od\n",
        "od.download(\"https://www.kaggle.com/datasets/crmercado/tweets-blogs-news-swiftkey-dataset-4million\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uo_HwBODSFEe",
        "outputId": "fed52791-a515-42f4-cdbc-35b611c4579d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: seshupavan\n",
            "Your Kaggle Key: ··········\n",
            "Downloading tweets-blogs-news-swiftkey-dataset-4million.zip to ./tweets-blogs-news-swiftkey-dataset-4million\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.10G/1.10G [00:12<00:00, 98.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import nltk\n",
        "import random\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "file_path = '/content/tweets-blogs-news-swiftkey-dataset-4million/final/en_US/en_US.twitter.txt'\n",
        "data_dir = '/path/to/nltk_data'\n",
        "nltk.data.path.append(data_dir)\n",
        "nltk.download('punkt')\n",
        "\n",
        "with open(file_path, \"r\") as f:\n",
        "    data = f.read()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3HWwdLASpHL",
        "outputId": "6d3ac957-882c-4cd5-a890-ae98354dcbfc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Cleaning"
      ],
      "metadata": {
        "id": "KSTSZu5nKDD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_pipeline(data):\n",
        "    \"Pre-processes and tokenizes the data\"\n",
        "    sentences = data.split('\\n')\n",
        "    sentences = [s.strip() for s in sentences]\n",
        "    sentences = [s for s in sentences if len(s) > 0]\n",
        "    tokenized = []\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.lower()\n",
        "        token = nltk.word_tokenize(sentence)\n",
        "        tokenized.append(token)\n",
        "    return tokenized\n",
        "\n",
        "tokenized_sentences = preprocess_pipeline(data)"
      ],
      "metadata": {
        "id": "d0HE3Gz_Tfkm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = train_test_split(tokenized_sentences, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "8VtZ2EY4WoL8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Pre-processing"
      ],
      "metadata": {
        "id": "V0uf1UU_KGDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_the_words(sentences):\n",
        "    \"Counts the occurance of the words in the corpus and returns key-value pairs\"\n",
        "    word_counts = {}\n",
        "    for sentence in sentences:\n",
        "        for token in sentence:\n",
        "            if token not in word_counts.keys():\n",
        "                word_counts[token] = 1\n",
        "            else:\n",
        "                word_counts[token] += 1\n",
        "    return word_counts\n",
        "\n",
        "def handling_oov(tokenized_sentences, count_threshold):\n",
        "    \"Find the words that appears n-times or more\"\n",
        "    closed_vocabulary = []\n",
        "    words_count = count_the_words(tokenized_sentences)\n",
        "    for word, count in words_count.items():\n",
        "        if count >= count_threshold:\n",
        "            closed_vocabulary.append(word)\n",
        "    return closed_vocabulary\n",
        "\n",
        "def replace_oov_words_by_unk(tokenized_sentences, vocabulary, unknown_token=\"<unk>\"):\n",
        "    \"Replaces the unknown words with <unk> token\"\n",
        "    vocabulary = set(vocabulary)\n",
        "    new_tokenized_words = []\n",
        "    for sentence in tokenized_sentences:\n",
        "        replaced_sentence = []\n",
        "        for token in sentence:\n",
        "            if token in vocabulary:\n",
        "                replaced_sentence.append(token)\n",
        "            else:\n",
        "                replaced_sentence.append(unknown_token)\n",
        "        new_tokenized_words.append(replaced_sentence)\n",
        "    return new_tokenized_words"
      ],
      "metadata": {
        "id": "we95bq25Xb7u"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(train_data, test_data, count_threshold):\n",
        "    vocabulary = handling_oov(train_data, count_threshold)\n",
        "    train_data_replaced = replace_oov_words_by_unk(train_data,vocabulary)\n",
        "    test_data_replaced = replace_oov_words_by_unk(test_data,vocabulary)\n",
        "    return train_data_replaced, test_data_replaced, vocabulary\n",
        "\n",
        "min_freq = 3\n",
        "final_train, final_test, vocabulary = preprocess_data(train_data, test_data, min_freq)"
      ],
      "metadata": {
        "id": "U86F53oO7YeG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sample data\n",
        "print(f\"The length of training data is {len(final_train)} and first preprocessed sample is:\")\n",
        "print(final_train[0])\n",
        "print(f\"The length of testing data is {len(final_test)} and first preprocessed sample is:\")\n",
        "print(final_test[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpPnFtp49UAs",
        "outputId": "0ba39436-e638-42e6-8695-3144e8dfb04d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The length of training data is 1888118 and first preprocessed sample is:\n",
            "['madison', 'would', 'go', 'into', 'a', 'coma', 'reading', 'what', 'i', 'say', 'about', 'him', '...']\n",
            "The length of testing data is 472030 and first preprocessed sample is:\n",
            "['seems', 'difficult', 'to', 'find', 'people', 'on', 'twitter', '.', 'found', 'nytimes', ',', '<unk>', ',', 'but', 'mainly', 'hit', 'and', 'miss', '...', 'any', 'suggestions', 'for', 'finding', 'content', 'sources']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building N-gram model"
      ],
      "metadata": {
        "id": "7b5W7_3WKJa5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_n_grams(data, n, start_token='<s>', end_token = '<e>'):\n",
        "    n_grams = {}\n",
        "    for sentence in data:\n",
        "        sentence = [start_token]*n + sentence + [end_token]\n",
        "        sentence = tuple(sentence)\n",
        "        m = len(sentence) if n == 1 else len(sentence) - 1\n",
        "        for i in range(m):\n",
        "            n_gram = sentence[i:i+n]\n",
        "            if n_gram in n_grams.keys():\n",
        "                n_grams[n_gram] += 1\n",
        "            else:\n",
        "                n_grams[n_gram] = 1\n",
        "    return n_grams\n",
        "\n",
        "#test-code\n",
        "sentences = [[\"Today\", \"is\", \"sunday\"], [\"sunday\", \"is\", \"holiday\"]]\n",
        "print(f\"unigram, {count_n_grams(sentences, 1)}\")\n",
        "print(f\"Bigram, {count_n_grams(sentences, 2)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ISIxWNF985a",
        "outputId": "4bd6e3da-843f-4a10-e247-0a2c6e94ac57"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unigram, {('<s>',): 2, ('Today',): 1, ('is',): 2, ('sunday',): 2, ('<e>',): 2, ('holiday',): 1}\n",
            "Bigram, {('<s>', '<s>'): 2, ('<s>', 'Today'): 1, ('Today', 'is'): 1, ('is', 'sunday'): 1, ('sunday', '<e>'): 1, ('<s>', 'sunday'): 1, ('sunday', 'is'): 1, ('is', 'holiday'): 1, ('holiday', '<e>'): 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_probability(word, previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
        "    \"calculate probability of a next word using n-gram counts with k-smooting\"\n",
        "    previous_n_gram = tuple(previous_n_gram)\n",
        "    previous_n_gram_count = n_gram_counts[previous_n_gram] if previous_n_gram in n_gram_counts else 0\n",
        "    denominator = previous_n_gram_count + k * vocabulary_size\n",
        "    n_plus1_gram = previous_n_gram + (word,)\n",
        "    n_plus1_gram_count = n_plus1_gram_counts[n_plus1_gram] if n_plus1_gram in n_plus1_gram_counts else 0\n",
        "    numerator = n_plus1_gram_count + k\n",
        "    probability = numerator / denominator\n",
        "    return probability\n",
        "\n",
        "def estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0):\n",
        "    \"Estimate the probabilities of next words using the n-gram counts with k-smoothing\"\n",
        "    previous_n_gram = tuple(previous_n_gram)\n",
        "    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
        "    vocabulary_size = len(vocabulary)\n",
        "    probabilities = {}\n",
        "    for word in vocabulary:\n",
        "        probability = estimate_probability(word, previous_n_gram,\n",
        "                                           n_gram_counts, n_plus1_gram_counts,\n",
        "                                           vocabulary_size, k=k)\n",
        "        probabilities[word] = probability\n",
        "    return probabilities"
      ],
      "metadata": {
        "id": "bmhChMmADS7I"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Auto Complete System"
      ],
      "metadata": {
        "id": "sk-c4v9aG7Vs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def suggest_a_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0, start_with=None):\n",
        "    \"Suggest a next word based on the probability\"\n",
        "    n = len(list(n_gram_counts.keys())[0])\n",
        "    previous_n_gram = previous_tokens[-n:]\n",
        "    probabilities = estimate_probabilities(previous_n_gram,\n",
        "                                           n_gram_counts, n_plus1_gram_counts,\n",
        "                                           vocabulary, k=k)\n",
        "    suggestions = None\n",
        "    max_prob = 0\n",
        "    for word, prob in probabilities.items():\n",
        "        if start_with:\n",
        "            if not word.startswith(start_with):\n",
        "                continue\n",
        "        if prob > max_prob:\n",
        "            suggestion = word\n",
        "            max_prob = prob\n",
        "    return suggestion, max_prob\n",
        "\n",
        "\n",
        "def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=None):\n",
        "    model_counts = len(n_gram_counts_list)\n",
        "    suggestions = []\n",
        "    for i in range(model_counts-1):\n",
        "        n_gram_counts = n_gram_counts_list[i]\n",
        "        n_plus1_gram_counts = n_gram_counts_list[i+1]\n",
        "        suggestion = suggest_a_word(previous_tokens, n_gram_counts,\n",
        "                                    n_plus1_gram_counts, vocabulary,\n",
        "                                    k=k, start_with=start_with)\n",
        "        suggestions.append(suggestion)\n",
        "    return suggestion"
      ],
      "metadata": {
        "id": "G0pOXqFWFhay"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing"
      ],
      "metadata": {
        "id": "UpTIvq9WJhJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_gram_counts_list = []\n",
        "for n in range(1, 6):\n",
        "    n_model_counts = count_n_grams(final_train, n)\n",
        "    n_gram_counts_list.append(n_model_counts)"
      ],
      "metadata": {
        "id": "Ez73N5mZJS4e"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "previous_tokens = [\"hey\", \"how\", \"are\", \"you\", \"today\"]\n",
        "suggestions_01 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
        "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
        "display(suggestions_01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "cyUUZIB8KQK2",
        "outputId": "7289b1df-c554-4331-d8ab-0ffaf792b2bb"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The previous words are ['hey', 'how', 'are', 'you', 'today'], the suggestions are:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "('?', 0.0007301070494795521)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "previous_tokens = [\"hey\", \"how\", \"are\"]\n",
        "suggestion_02 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
        "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
        "display(suggestion_02)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "bG45BP2BLgbA",
        "outputId": "29b5e44c-2c57-43c2-e581-2ed540e0a5b1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The previous words are ['hey', 'how', 'are'], the suggestions are:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "('madison', 9.87615304086752e-06)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}